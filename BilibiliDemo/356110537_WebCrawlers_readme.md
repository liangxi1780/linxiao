#WebCrawlers

#通用搜索引擎局限：
(1) 不同领域、不同背景的用户往往具有不同的检索目的和需求，通用搜索引擎所返回的结果包含大量用户不关心的网页。
(2) 通用搜索引擎的目标是尽可能大的网络覆盖率，有限的搜索引擎服务器资源与无限的网络数据资源之间的矛盾将进一步加深。
(3) 万维网数据形式的丰富和网络技术的不断发展，图片、数据库、音频、视频多媒体等不同数据大量出现，通用搜索引擎往往对这些信息含量密集且具有一定结构的数据无能为力，不能很好地发现和获取。
(4) 通用搜索引擎大多提供基于关键字的检索，难以支持根据语义信息提出的查询。

#爬虫需要解决三个主要问题：
(1) 对抓取目标的描述或定义；
(2) 对网页或数据的分析与过滤；
(3) 对URL的搜索策略。

#五种表示页面质量高低的方式
(1) Similarity（页面与爬行主题之间的相似度）
(2) Backlink（页面在 Web 图中的入度大小）
(3) PageRank（指向它的所有页面平均权值之和）
(4) Forwardlink（页面在 Web 图中的出度大小）
(5) Location（页面的信息位置）

#Parallel（并行性问题）
(1) 重复性（并行运行的爬虫或爬行线程同时运行时增加了重复页面）
(2) 质量问题（并行运行时，每个爬虫或爬行线程只能获取部分页面，导致页面质量下降）
(3) 通信带宽代价（并行运行时，各个爬虫或爬行线程之间不可避免要进行一些通信）

# 并行运行时，网络爬虫通常采用三种方式：
(1) 独立方式（各个爬虫独立爬行页面，互不通信）
(2) 动态分配方式（由一个中央协调器动态协调分配 URL 给各个爬虫）
(3) 静态分配方式（URL 事先划分给各个爬虫）

#网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型：
(1) 通用网络爬虫（General Purpose Web Crawler）
(2) 聚焦网络爬虫（Focused Web Crawler）
(3) 增量式网络爬虫（Incremental Web Crawler）
(4) 深层网络爬虫（Deep Web Crawler）

实际的网络爬虫系统通常是几种爬虫技术相结合实现的

#通用网络爬虫
通用网络爬虫又称全网爬虫（Scalable Web Crawler），爬行对象从一些种子 URL 扩充到整个 Web，主要为门户站点搜索引擎和大型 Web 服务提供商采集数据。
结构大致可以分为页面爬行模块 、页面分析模块、链接过滤模块、页面数据库、URL 队列、初始 URL 集合。

常用的爬行策略有：深度优先策略、广度优先策略
1) 深度优先策略：
	其基本方法是按照深度由低到高的顺序，依次访问下一级网页链接，直到不能再深入为止。 爬虫在完成一个爬行分支后返回到上一链接节点进一步搜索其它链接。 当所有链接遍历完后，爬行任务结束。 这种策略比较适合垂直搜索或站内搜索， 但爬行页面内容层次较深的站点时会造成资源的巨大浪费。

2) 广度优先策略：
	此策略按照网页内容目录层次深浅来爬行页面，处于较浅目录层次的页面首先被爬行。 当同一层次中的页面爬行完毕后，爬虫再深入下一层继续爬行。 这种策略能够有效控制页面的爬行深度，避免遇到一个无穷深层分支时无法结束爬行的问题，实现方便，无需存储大量中间节点，不足之处在于需较长时间才能爬行到目录层次较深的页面。

#聚焦网络爬虫
聚焦网络爬虫（Focused Crawler），又称主题网络爬虫（Topical Crawler），是指选择性地爬行那些与预先定义好的主题相关页面的网络爬虫。
只需要爬行与主题相关的页面，极大地节省了硬件和网络资源，保存的页面也由于数量少而更新快，还可以很好地满足一些特定人群对特定领域信息的需求。
和通用网络爬虫相比，增加了链接评价模块以及内容评价模块。聚焦爬虫爬行策略实现的关键是评价页面内容和链接的重要性，不同的方法计算出的重要性不同，由此导致链接的访问顺序也不同。

1) 基于内容评价的爬行策略：
	Fish Search 算法，它将用户输入的查询词作为主题，包含查询词的页面被视为与主题相关，其局限性在于无法评价页面与主题相关度的高低 。
	Sharksearch 算法，利用空间向量模型计算页面与主题的相关度大小。

2) 基于链接结构评价的爬行策略 ：
	Web 页面作为一种半结构化文档，包含很多结构信息，可用来评价链接重要性。 
	PageRank 算法最初用于搜索引擎信息检索中对查询结果进行排序，也可用于评价链接重要性，具体做法就是每次选择 PageRank 值较大页面中的链接来访问。 
	另一个利用 Web结构评价链接价值的方法是 HITS 方法，它通过计算每个已访问页面的 Authority 权重和 Hub 权重，并以此决定链接的访问顺序。

3) 基于增强学习的爬行策略：
	利用贝叶斯分类器，根据整个网页文本和链接文本对超链接进行分类，为每个链接计算出重要性，从而决定链接的访问顺序。

4) 基于语境图的爬行策略：
	通过建立语境图（Context Graphs）学习网页之间的相关度，训练一个机器学习系统，通过该系统可计算当前页面到相关 Web 页面的距离，距离越近的页面中的链接优先访问。
	印度理工大学（IIT）和 IBM 研究中心的研究人员开发了一个典型的聚焦网络爬虫。 该爬虫对主题的定义既不是采用关键词也不是加权矢量，而是一组具有相同主题的网页。 
	它包含两个重要模块：
	【a】一个是分类器，用来计算所爬行的页面与主题的相关度，确定是否与主题相关；
	【b】另一个是净化器，用来识别通过较少链接连接到大量相关页面 的中心页面 。
	
#增量式网络爬虫
增量式网络爬虫（IncrementalWebCrawler）是指对已下载网页采取增量式更新和只爬行新产生的或者已经发生变化网页的爬虫，它能够在一定程度上保证所爬行的页面是尽可能新的页面。

增量式网络爬虫的体系结构包含:爬行模块、排序模块、更新模块、本地页面集、待爬行 URL 集以及本地页面 URL 集

增量式爬虫有两个目标：保持本地页面集中存储的页面为最新页面和提高本地页面集中页面的质量。

第一个目标，增量式爬虫需要通过重新访问网页来更新本地页面集中页面内容，常用的方法有：
 1) 统一更新法：爬虫以相同的频率访问所有网页，不考虑网页的改变频率；
 2) 个体更新法：爬虫根据个体网页的改变频率来重新访问各页面；
 3) 基于分类的更新法：爬虫根据网页改变频率将其分为更新较快网页子集和更新较慢网页子集两类，然后以不同的频率访问这两类网页
 
第二个目标，增量式爬虫需要对网页的重要性排序，常用的策略有：广度优先策略、PageRank 优先策略等。 

#Deep Web 爬虫
Web 页面按存在方式可以分为表层网页（Surface Web）和深层网页（Deep Web，也称 Invisible Web Pages 或 Hidden Web）。 
表层网页是指传统搜索引擎可以索引的页面，以超链接可以到达的静态网页为主构成的 Web 页面。
Deep Web 是那些大部分内容不能通过静态链接获取的、隐藏在搜索表单后的，只有用户提交一些关键词才能获得的 Web 页面。例如那些用户注册后内容才可见的网页就属于 Deep Web。

Deep Web 爬虫体系结构包含
##六个基本功能模块 ：
###爬行控制器
###解析器
###表单分析器
###表单处理器
###响应分析器
###LVS 控制器

##两个爬虫内部数据结构：
###URL 列表
###LVS 表，LVS（Label Value Set）表示标签/数值集合，用来表示填充表单的数据源

Deep Web 爬虫爬行过程中最重要部分就是表单填写，包含两种类型：
1) 基于领域知识的表单填写：
	此方法一般会维持一个本体库，通过语义分析来选取合适的关键词填写表单。 

2) 基于网页结构分析的表单填写： 
	此方法一般无领域知识或仅有有限的领域知识，将网页表单表示成 DOM 树，从中提取表单各字段值。 

#抓取目标的描述和定义
抓取目标的描述和定义是决定网页分析算法与URL搜索策略如何制订的基础。而网页分析算法和候选URL排序算法是决定搜索引擎所提供的服务形式和爬虫网页抓取行为的关键所在。这两个部分的算法又是紧密相关的。

聚焦爬虫对抓取目标的描述可分为：
1) 基于目标网页特征
2) 基于目标数据模式
3) 基于领域概念

##基于目标网页特征
基于目标网页特征的爬虫所抓取、存储并索引的对象一般为网站或网页。根据种子样本获取方式可分为：
(1) 预先给定的初始抓取种子样本；
(2) 预先给定的网页分类目录和与分类目录对应的种子样本 
(3) 通过用户行为确定的抓取目标样例，分为：
	(a) 用户浏览过程中显示标注的抓取样本；
	(b) 通过用户日志挖掘得到访问模式及相关样本。
其中，网页特征可以是网页的内容特征，也可以是网页的链接结构特征，等等。

##基于目标数据模式
基于目标数据模式的爬虫针对的是网页上的数据，所抓取的数据一般要符合一定的模式，或者可以转化或映射为目标数据模式。

##基于领域概念
另一种描述方式是建立目标领域的本体或词典，用于从语义角度分析不同特征在某一主题中的重要程度。

#网页搜索策略
网页的抓取策略可以分为深度优先、广度优先和最佳优先三种。深度优先在很多情况下会导致爬虫的陷入(trapped)问题，目前常见的是广度优先和最佳优先方法。

##广度优先搜索
广度优先搜索策略是指在抓取过程中，在完成当前层次的搜索后，才进行下一层次的搜索。
该算法的设计和实现相对简单。在目前为覆盖尽可能多的网页，一般使用广度优先搜索方法。也有很多研究将广度优先搜索策略应用于聚焦爬虫中。其基本思想是认为与初始URL在一定链接距离内的网页具有主题相关性的概率很大。另外一种方法是将广度优先搜索与网页过滤技术结合使用，先用广度优先策略抓取网页，再将其中无关的网页过滤掉。这些方法的缺点在于，随着抓取网页的增多，大量的无关网页将被下载并过滤，算法的效率将变低。 

##最佳优先搜索
最佳优先搜索策略按照一定的网页分析算法，预测候选URL与目标网页的相似度，或与主题的相关性，并选取评价最好的一个或几个URL进行抓取。
它只访问经过网页分析算法预测为“有用”的网页。存在的一个问题是，在爬虫抓取路径上的很多相关网页可能被忽略，因为最佳优先策略是一种局部最优搜索算法。因此需要将最佳优先结合具体的应用进行改进，以跳出局部最优点。 研究表明，这样的闭环调整可以将无关网页数量降低30%~90%。

##深度优先搜索
深度优先搜索策略从起始网页开始，选择一个URL进入，分析这个网页中的URL，选择一个再进入。如此一个链接一个链接地抓取下去，直到处理完一条路线之后再处理下一条路线。深度优先策略设计较为简单。然而门户网站提供的链接往往最具价值，PageRank也很高，但每深入一层，网页价值和PageRank都会相应地有所下降。这暗示了重要网页通常距离种子较近，而过度深入抓取到的网页却价值很低。同时，这种策略抓取深度直接影响着抓取命中率以及抓取效率，对抓取深度是该种策略的关键。相对于其他两种策略而言。此种策略很少被使用。

#网页分析算法
##基于网络拓扑
基于网页之间的链接，通过已知的网页或数据，来对与其有直接或间接链接关系的对象（可以是网页或网站等）作出评价的算法。
###网页(Webpage)粒度的分析算法
PageRank和HITS算法是最常见的链接分析算法，两者都是通过对网页间链接度的递归和规范化计算，得到每个网页的重要度评价。
PageRank算法虽然考虑了用户访问行为的随机性和Sink网页的存在，但忽略了绝大多数用户访问时带有目的性，即网页和链接与查询主题的相关性。
针对这个问题，HITS算法提出了两个关键的概念：权威型网页（authority）和中心型网页（hub）
基于链接的抓取的问题是相关页面主题团之间的隧道现象，即很多在抓取路径上偏离主题的网页也指向目标网页，局部评价策略中断了在当前路径上的抓取行为。

###网站粒度的分析算法
网站粒度的资源发现和管理策略也比网页粒度的更简单有效。网站粒度的爬虫抓取的关键之处在于站点的划分和站点等级(SiteRank)的计算。SiteRank的计算方法与PageRank类似，但是需要对网站之间的链接作一定程度抽象，并在一定的模型下计算链接的权重。
网站划分情况分为按域名划分和按IP地址划分两种。通过对同一个域名下不同主机、服务器的IP地址进行站点划分，构造站点图，利用类似PageRank的方法评价SiteRank。同时，根据不同文件在各个站点上的分布情况，构造文档图，结合SiteRank分布式计算得到DocRank。利用分布式的SiteRank计算，不仅大大降低了单机站点的算法代价，而且克服了单独站点对整个网络覆盖率有限的缺点。附带的一个优点是，常见PageRank 造假难以对SiteRank进行欺骗。

###网页块粒度的分析算法
在一个页面中，往往含有多个指向其他页面的链接，这些链接中只有一部分是指向主题相关网页的，或根据网页的链接锚文本表明其具有较高重要性。但是，在PageRank和HITS算法中，没有对这些链接作区分，因此常常给网页分析带来广告等噪声链接的干扰。在网页块级别(Block?level)进行链接分析的算法的基本思想是通过VIPS网页分割算法将网页分为不同的网页块(page block)，然后对这些网页块建立page?to?block和block?to?page的链接矩阵，?分别记为Z和X。于是，在page?to?page图上的网页块级别的PageRank为?W?p=X×Z；?在block?to?block图上的BlockRank为?W?b=Z×X。已经有人实现了块级别的PageRank和HITS算法，并通过实验证明，效率和准确率都比传统的对应算法要好。

##基于网页内容

##基于用户访问行为
基于网页内容的分析算法指的是利用网页内容（文本、数据等资源）特征进行的网页评价。
网页的内容从原来的以超文本为主，发展到后来动态页面（或称为Hidden Web）数据为主，后者的数据量约为直接可见页面数据（PIW，Publicly Indexable Web）的400~500倍。另一方面，多媒体数据、Web Service等各种网络资源形式也日益丰富。因此，基于网页内容的分析算法也从原来的较为单纯的文本检索方法，发展为涵盖网页数据抽取、机器学习、数据挖掘、语义理解等多种方法的综合应用。

###根据网页数据形式的不同，将基于网页内容的分析算法，归纳以下三类：
第一种针对以文本和超链接为主的无结构或结构很简单的网页；
第二种针对从结构化的数据源（如RDBMS）动态生成的页面，其数据不能直接批量访问；
第三种针对的数据界于第一和第二类数据之间，具有较好的结构，显示遵循一定模式或风格，且可以直接访问。

###基于文本的网页分析算法
1) 纯文本分类与聚类算法
很大程度上借用了文本检索的技术。文本分析算法可以快速有效的对网页进行分类和聚类，但是由于忽略了网页间和网页内部的结构信息，很少单独使用。
2) 超文本分类和聚类算法
根据网页链接网页的相关类型对网页进行分类，依靠相关联的网页推测该网页的类型。

#网页爬虫的行为通常是四种策略组合的结果。
♦ 选择策略，决定所要下载的页面；
♦ 重新访问策略，决定什么时候检查页面的更新变化；
♦ 平衡礼貌策略，指出怎样避免站点超载；
♦ 并行策略，指出怎么协同达到分布式抓取的效果；











 # 良心友情链接

[腾讯QQ群快速检索](http://u.720life.cn/s/8cf73f7c)

[软件免费开发论坛](http://u.720life.cn/s/bbb01dc0)